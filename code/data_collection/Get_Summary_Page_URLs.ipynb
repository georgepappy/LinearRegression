{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0e84b46",
   "metadata": {},
   "source": [
    "## This notebook is used to scrape home detail page urls off the nine pages resulting from a targeted home search which yields summaries & links to homes matching that search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e557cd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import requests, bs4\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "import time, random\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205ff31a",
   "metadata": {},
   "source": [
    "# 1) Start by Grabbing the URLs off of Redfin.com: \n",
    "\n",
    "### Single Family Homes that have recently sold in the City of Los Angeles (first subset)\n",
    "\n",
    "#### (Note: Due to the website detecting & halting automated/repetitive activity, this should be done piecemeal - one subsearch at a time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "875d44b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Update the User-Agent per \n",
    "#     this StackOverflow post: https://stackoverflow.com/questions/38489386/python-requests-403-forbidden\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.131 Safari/537.36',\n",
    "           'Accept-Encoding': 'gzip, deflate, br', \n",
    "           'Accept': '*/*', \n",
    "           'Connection': 'keep-alive'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca1acedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = '<url for first summary page of targeted home search here>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c83cd217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Got page 1\n",
      "Success! Got page 2\n",
      "Success! Got page 3\n",
      "Success! Got page 4\n",
      "Success! Got page 5\n",
      "Success! Got page 6\n",
      "Success! Got page 7\n",
      "Success! Got page 8\n",
      "Success! Got page 9\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "for page_num in range(1,10):\n",
    "    if page_num == 1:\n",
    "        url = url_base\n",
    "    else:\n",
    "        url = url_base + '/page-' + str(page_num)\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    # Delay (possibly helpful to make sure response was received)\n",
    "    time.sleep(5)\n",
    "    status = response.status_code\n",
    "    if status == 200:\n",
    "        print('Success! Got page {}'.format(page_num))\n",
    "        page = response.text\n",
    "        soup = bs(page)\n",
    "        # Find and save urls of houses we want to scrape later        \n",
    "        homes_list = [el['href'] for el in soup.find_all('a', {'class': 'slider-item'})]\n",
    "        for home in homes_list:\n",
    "            with open('homes_list.txt', 'a') as f:\n",
    "                f.write(home + '\\n')\n",
    "    \n",
    "    else:\n",
    "        print('Fail! Received status code {} for page {}'.format(status, page_num))\n",
    "    \n",
    "    # Delay some more to linger on the page before moving on (35 +/- 5 seconds)\n",
    "    if page_num < 9:\n",
    "        time.sleep(35 + 10*(random.random() - 0.5))\n",
    "    \n",
    "print('DONE!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a8e3e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
